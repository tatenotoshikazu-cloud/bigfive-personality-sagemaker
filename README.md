# Big Fiveæ€§æ ¼ç‰¹æ€§æ¨å®šãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ

æ—¥æœ¬èªä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰Big Fiveæ€§æ ¼ç‰¹æ€§ï¼ˆé–‹æ”¾æ€§ã€èª å®Ÿæ€§ã€å¤–å‘æ€§ã€å”èª¿æ€§ã€ç¥çµŒç—‡å‚¾å‘ï¼‰ã‚’é«˜ç²¾åº¦ã§æ¨å®šã™ã‚‹AIãƒ¢ãƒ‡ãƒ«

## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦

- **ç›®çš„**: æ—¥æœ¬èªä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰Big Fiveæ€§æ ¼ç‰¹æ€§ã‚’æ¨å®š
- **å­¦ç¿’ãƒ‡ãƒ¼ã‚¿**: Nemotron-Personas-Japan + RealPersonaChatï¼ˆäºŒæ®µéšå­¦ç¿’ï¼‰
- **æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯**: AWS SageMaker + LoRA + xlm-roberta-large

## ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆ

```
DL/
â”œâ”€â”€ download_datasets.py      # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå–å¾—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”œâ”€â”€ preprocess_data.py         # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”œâ”€â”€ train.py                   # SageMakerå¯¾å¿œãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”œâ”€â”€ run_sagemaker.py           # SageMakerå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”œâ”€â”€ requirements.txt           # ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
â”œâ”€â”€ README.md                  # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«
â””â”€â”€ data/                      # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆè‡ªå‹•ç”Ÿæˆï¼‰
    â”œâ”€â”€ realpersonachat/       # RealPersonaChatãƒ‡ãƒ¼ã‚¿
    â”œâ”€â”€ nemotron/              # Nemotron-Personas-Japanãƒ‡ãƒ¼ã‚¿
    â””â”€â”€ processed/             # å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿
```

## ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

### 1. ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
pip install -r requirements.txt
```

### 2. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå–å¾—

```bash
python download_datasets.py
```

**å–å¾—ã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ:**
- **RealPersonaChat**: ç´„14,000ä»¶ã®æ—¥æœ¬èªå¯¾è©±ãƒ‡ãƒ¼ã‚¿ + Big Fiveç‰¹æ€§ãƒ©ãƒ™ãƒ«
- **Nemotron-Personas-Japan**: 1M-10Mä»¶ã®æ—¥æœ¬èªãƒšãƒ«ã‚½ãƒŠåˆæˆãƒ‡ãƒ¼ã‚¿

### 3. ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†

```bash
python preprocess_data.py
```

**å‡¦ç†å†…å®¹:**
- ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ¢ãƒ‡ãƒ«å…¥åŠ›å½¢å¼ã«å¤‰æ›
- Big Fiveç‰¹æ€§ãƒ©ãƒ™ãƒ«ã®æŠ½å‡º
- Stage 1ï¼ˆNemotronï¼‰/ Stage 2ï¼ˆRealPersonaChatï¼‰ç”¨ã«ãƒ‡ãƒ¼ã‚¿åˆ†å‰²

## ãƒ­ãƒ¼ã‚«ãƒ«å­¦ç¿’ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

SageMakerå®Ÿè¡Œå‰ã«ãƒ­ãƒ¼ã‚«ãƒ«ã§ãƒ†ã‚¹ãƒˆã™ã‚‹å ´åˆï¼š

```bash
# Stage 1å­¦ç¿’
python train.py --stage 1 --epochs 3 --batch_size 8

# Stage 2å­¦ç¿’
python train.py --stage 2 --epochs 5 --batch_size 8 --stage1_model_path output/final_model
```

## âœ… ãƒ­ãƒ¼ã‚«ãƒ«å‹•ä½œç¢ºèªæ¸ˆã¿

ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã¯**ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§å‹•ä½œç¢ºèªæ¸ˆã¿**ã§ã™ï¼š
- âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ï¼ˆRealPersonaChat: 500ä»¶ï¼‰
- âœ… Stage 1ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆNemotronè£œåŠ©ã‚¿ã‚¹ã‚¯å­¦ç¿’ï¼‰
- âœ… ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ï¼ˆå¹´é½¢ãƒ»æ€§åˆ¥ãƒ»è·æ¥­äºˆæ¸¬ï¼‰
- âœ… LoRA + xlm-roberta-large ãƒ¢ãƒ‡ãƒ«

**ãƒ­ãƒ¼ã‚«ãƒ«ãƒ†ã‚¹ãƒˆã‚³ãƒãƒ³ãƒ‰:**
```bash
# Stage 1å­¦ç¿’ï¼ˆå‹•ä½œç¢ºèªï¼‰
python train_stage1_local.py

# Stage 2å­¦ç¿’ï¼ˆå‹•ä½œç¢ºèªï¼‰
python train_stage2_local.py
```

## ğŸš€ AWS SageMakerå®Ÿè¡Œï¼ˆæœ¬ç•ªï¼‰

ãƒ­ãƒ¼ã‚«ãƒ«ã§å‹•ä½œç¢ºèªã—ãŸã‚³ãƒ¼ãƒ‰ã‚’SageMakerä¸Šã§å®Ÿè¡Œã—ã¾ã™ã€‚

**è©³ç´°ã¯ [SAGEMAKER_GUIDE.md](SAGEMAKER_GUIDE.md) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚**

### ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

1. **AWSèªè¨¼è¨­å®š**
   ```bash
   aws configure
   ```

2. **è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ç·¨é›†**

   [run_sagemaker.py](run_sagemaker.py) ã®206-209è¡Œç›®ã‚’ç·¨é›†ï¼š
   ```python
   ROLE_ARN = 'arn:aws:iam::YOUR_ACCOUNT_ID:role/YOUR_SAGEMAKER_ROLE'
   BUCKET_NAME = 'your-bigfive-bucket'
   REGION = 'us-west-2'
   ```

3. **SageMakerå®Ÿè¡Œ**
   ```python
   # run_sagemaker.py ã®262è¡Œç›®ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã‚’è§£é™¤
   python run_sagemaker.py
   ```

### å®Ÿè¡Œãƒ•ãƒ­ãƒ¼

1. ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ â†’ S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
2. **Stage 1å­¦ç¿’**ï¼ˆNemotronè£œåŠ©ã‚¿ã‚¹ã‚¯ï¼‰
   - æ¨å®šæ™‚é–“: 3-5æ™‚é–“
   - ã‚³ã‚¹ãƒˆ: ç´„$2.21-$3.68
3. **Stage 2å­¦ç¿’**ï¼ˆBig Fiveäºˆæ¸¬ï¼‰
   - æ¨å®šæ™‚é–“: 4-6æ™‚é–“
   - ã‚³ã‚¹ãƒˆ: ç´„$2.94-$4.42

### æ¨å¥¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹

- **ml.g4dn.xlarge** (æ¨å¥¨): NVIDIA T4 GPUã€$0.736/æ™‚é–“
- ml.p3.2xlarge: NVIDIA V100 GPUã€é«˜é€Ÿã ãŒé«˜ã‚³ã‚¹ãƒˆ
- ml.g5.xlarge: NVIDIA A10G GPUã€æœ€æ–°ä¸–ä»£

## å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

### LoRAè¨­å®š

```python
lora_r = 16              # LoRAã®ãƒ©ãƒ³ã‚¯
lora_alpha = 32          # LoRAã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°
lora_dropout = 0.1       # Dropoutç‡
```

### å­¦ç¿’è¨­å®š

**Stage 1ï¼ˆNemotronï¼‰:**
- Epochs: 3
- Batch Size: 8
- Learning Rate: 2e-4

**Stage 2ï¼ˆRealPersonaChatï¼‰:**
- Epochs: 5
- Batch Size: 8
- Learning Rate: 1e-4ï¼ˆStage 1ã‚ˆã‚Šä½ã‚ï¼‰

## Big Fiveç‰¹æ€§

ãƒ¢ãƒ‡ãƒ«ã¯ä»¥ä¸‹ã®5æ¬¡å…ƒã‚’æ¨å®šã—ã¾ã™ï¼š

1. **Opennessï¼ˆé–‹æ”¾æ€§ï¼‰**: æ–°ã—ã„çµŒé¨“ã¸ã®é–‹æ”¾åº¦
2. **Conscientiousnessï¼ˆèª å®Ÿæ€§ï¼‰**: è¨ˆç”»æ€§ãƒ»è²¬ä»»æ„Ÿ
3. **Extraversionï¼ˆå¤–å‘æ€§ï¼‰**: ç¤¾äº¤æ€§ãƒ»æ´»ç™ºæ€§
4. **Agreeablenessï¼ˆå”èª¿æ€§ï¼‰**: å”èª¿æ€§ãƒ»æ€ã„ã‚„ã‚Š
5. **Neuroticismï¼ˆç¥çµŒç—‡å‚¾å‘ï¼‰**: æƒ…ç·’ä¸å®‰å®šæ€§

## è©•ä¾¡æŒ‡æ¨™

- **MSEï¼ˆMean Squared Errorï¼‰**: å¹³å‡äºŒä¹—èª¤å·®
- **MAEï¼ˆMean Absolute Errorï¼‰**: å¹³å‡çµ¶å¯¾èª¤å·®
- å„Big Fiveæ¬¡å…ƒã”ã¨ã®MAE

## ã‚³ã‚¹ãƒˆè¦‹ç©ã‚‚ã‚Šï¼ˆAWS SageMakerï¼‰

### Stage 1å­¦ç¿’ï¼ˆNemotronï¼‰
- ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹: ml.g4dn.xlarge
- å­¦ç¿’æ™‚é–“: ç´„6-10æ™‚é–“ï¼ˆãƒ‡ãƒ¼ã‚¿é‡ã«ä¾å­˜ï¼‰
- ã‚³ã‚¹ãƒˆ: ç´„$5-8

### Stage 2å­¦ç¿’ï¼ˆRealPersonaChatï¼‰
- ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹: ml.g4dn.xlarge
- å­¦ç¿’æ™‚é–“: ç´„2-3æ™‚é–“
- ã‚³ã‚¹ãƒˆ: ç´„$2-3

**åˆè¨ˆè¦‹ç©ã‚‚ã‚Š**: $7-11ï¼ˆ1å›ã®å®Œå…¨å­¦ç¿’ï¼‰

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼

```python
# ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ã§å–å¾—ï¼ˆãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰
dataset = load_dataset("nvidia/Nemotron-Personas-Japan", streaming=True)
```

### GPU ãƒ¡ãƒ¢ãƒªä¸è¶³

```python
# ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ¸›ã‚‰ã™
--batch_size 4

# Gradient Accumulationã‚’ä½¿ç”¨
--gradient_accumulation_steps 2
```

### SageMakerèªè¨¼ã‚¨ãƒ©ãƒ¼

1. IAMãƒ­ãƒ¼ãƒ«ã«ä»¥ä¸‹ã®ãƒãƒªã‚·ãƒ¼ã‚’ã‚¢ã‚¿ãƒƒãƒï¼š
   - AmazonSageMakerFullAccess
   - AmazonS3FullAccess

2. ä¿¡é ¼é–¢ä¿‚ã« `sagemaker.amazonaws.com` ã‚’è¿½åŠ 

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. **ãƒ¢ãƒ‡ãƒ«è©•ä¾¡**: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®æ€§èƒ½è©•ä¾¡
2. **ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**: SageMaker Automatic Model Tuning
3. **ãƒ‡ãƒ—ãƒ­ã‚¤**: SageMakerã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆä½œæˆ
4. **æ¨è«–API**: Lambda + API Gatewayæ§‹æˆ

## å‚è€ƒæ–‡çŒ®

- **RealPersonaChat**: Yamashita et al. (2023) "RealPersonaChat: A Realistic Persona Chat Corpus with Interlocutors' Own Personalities"
- **Nemotron-Personas-Japan**: NVIDIA NeMo Data Designer
- **xlm-roberta-large**: Conneau et al. (2020) "Unsupervised Cross-lingual Representation Learning at Scale"
- **LoRA**: Hu et al. (2021) "LoRA: Low-Rank Adaptation of Large Language Models"

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

- RealPersonaChat: MIT License
- Nemotron-Personas-Japan: CC BY 4.0
- xlm-roberta-large: MIT License

## ãŠå•ã„åˆã‚ã›

ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«é–¢ã™ã‚‹è³ªå•ã‚„ææ¡ˆã¯ã€Issueã¾ã§ãŠé¡˜ã„ã—ã¾ã™ã€‚
